{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of basic Web Scraping\n",
    "\n",
    "Here is the deal: learn in one page how to scrape your facebook contacts to retrieve all their birthday dates and structure the data! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the desired list of birthday dates is not provided by facebook for download into a spreadsheet this is an example of unstructured data, and in this case we will **scrape** it from the web. Some websites have API (Application Programming Interface) that allow a user to retrieve structured data, for example directly querying their database. Levels and contraints of access depend on the specific website, and although facebook has an API we want to demonstrate here the skill of scraping data which is often useful in Data Science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic concept is that data can be retrieved from a webpage programmatically by parsing the source-code (e.g. HTML). Since we might want to do this automatically on many pages we need to **access pages programmatically** via their URL and possibly move from page to page by jumping from one URL to the next.\n",
    "\n",
    "The simplest tool to access URLs in Python is **urllib3** or **requests**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Web requests and BeautifulSoup\n",
    "\n",
    "Here is how to submit a request with the *requests* library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://www.google.ca/'\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have made a request to get the URL, we need to look at the content, and we can use the method r.content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!doctype html><html itemscope=\"\" itemtype=\"http://schema.org/WebPage\" lang=\"en-CA\"><head><meta content=\"text/html; charset=UTF-8\" http-equiv=\"Content-Type\"><meta content=\"/images/branding/googleg/1x/'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.content[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better way to visualize and navigate the source code is to use the BeaufitulSoup library. So we can create an object that we will call *soup* and call the method *prettify()* that will rearrange the code in its readable form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html itemscope=\"\" itemtype=\"http://schema.org/WebPage\" lang=\"en-CA\">\n",
      " <head>\n",
      "  <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
      "  <meta content=\"/images/branding/googleg/1x/googleg_standard_color_128dp.png\" itemprop=\"image\"/>\n",
      "  <title>\n",
      "   Google\n",
      "  </title>\n",
      "  <script>\n",
      "   (function(){window.google={kEI:'Hho8Wb2bN8K_jwTjk4XIBQ',kEXPI:'1353382,1353799,3700324,37003\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(r.content, \"lxml\")\n",
    "print(soup.prettify()[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need for trying other libraries\n",
    "\n",
    "What I first noticed is that the code returned from this kind of request is not identical to what I get by manually navigating to a page and using crtl+U to retrieve the code. In my case I want to see all the data that I see on my facebook page and the above method was not doing that. \n",
    "\n",
    "So I tried **urllib3** that can be used in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import the library used to query a website\n",
    "import urllib3\n",
    "import certifi\n",
    "\n",
    "http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED', ca_certs=certifi.where())\n",
    "url = 'https://www.google.ca/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page = http.request('GET', url)\n",
    "html = page.read()\n",
    "html = str(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html itemscope=\"\" itemtype=\"http://schema.org/WebPage\" lang=\"en-CA\">\n",
      " <head>\n",
      "  <met\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(page.data, \"lxml\")\n",
    "print(soup.prettify()[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again one can retrieve the source code, but you still don't get the same info as you manually navigate with your browser, this is because the request is not sent as when you do with the browser, and the website *recognizes* that is a bot request. Apparently **urllib3** offers the option to extend the request with an *header* that refers to a browser, and even if this still didn't do what I wanted here is some example code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "url = \"\"\"https://www.facebook.com/\"\"\"\n",
    "req = urllib.request.Request(\n",
    "    url, \n",
    "    data=None, \n",
    "    headers={\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'\n",
    "    }\n",
    ")\n",
    "\n",
    "f = urllib.request.urlopen(req)\n",
    "test = f.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we need to find a way to simulate a browser and get an identical behaviour.\n",
    "\n",
    "For this purpose a powerful tool is the **Selenium** library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Success: Webdriver from the Selenium library\n",
    "\n",
    "This actually worked, in the sense that inspecting the result obtained with the Selenium library, I could find the same code that I would obtain manually. So here is how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver  \n",
    "from selenium.common.exceptions import NoSuchElementException  \n",
    "from selenium.webdriver.common.keys import Keys  \n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "_chrome_options = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"https://www.facebook.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(chrome_options=_chrome_options)\n",
    "driver.get(url)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code successfully opens the Chrome browser and navigates to the desired URL. Now we just have to get the page_source and save it in a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "html_source = driver.page_source  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"\" id=\"facebook\" lang=\"en\" xmlns=\"http://www.w3.org/1999/xhtml\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"origin-when-crossorigin\" id=\"meta_referrer\" name=\"referr\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html_source,'html.parser')\n",
    "print(soup.prettify()[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of facebook, this code will not open my profile, but will accually navigate to the login page, therefore we will have to instruct the **webdriver** to insert our credentials and then navigate to the pages we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(chrome_options=_chrome_options) \n",
    "driver.get(url)  \n",
    "\n",
    "# wait for the login elements to load\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# identify relevant elements\n",
    "email = driver.find_element_by_css_selector('input[type=email]')\n",
    "password = driver.find_element_by_css_selector('input[type=password]')\n",
    "login = driver.find_element_by_css_selector('input[value=\"Log In\"]')\n",
    "\n",
    "# insert the credentials (you will have to modify this)\n",
    "email.send_keys('name.lastname@gmail.com')\n",
    "password.send_keys('your_password')\n",
    "\n",
    "# login\n",
    "login.click()\n",
    "\n",
    "# navigate to my friends list (you will have to get the right url for you)\n",
    "driver.get('https://www.facebook.com/profile.php?id=.....blabla......._friends_tl')\n",
    "\n",
    "# take ascreenshot\n",
    "#driver.get_screenshot_as_file('yourName-profile.png')\n",
    "\n",
    "# get source-code from current page\n",
    "html_source = driver.page_source  \n",
    "\n",
    "# quit browser\n",
    "#driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"\" id=\"facebook\" lang=\"it\" xmlns=\"http://www.w3.org/1999/xhtml\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"origin-when-crossorigin\" id=\"meta_referrer\" name=\"referr\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html_source,'html.parser')\n",
    "print(soup.prettify()[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this tool we can now navigate facebook as we were logged in our account. To go from one page to another we just need to figure out where to find the links. If I want to navigate from my profile to my friends list page, I need to parse the source code and find the URL that corresponds to that link. It is now a matter of finding the logic of where those URLs are located within the page, extract them from the soup with **regex** (Regular Expressions) and use another request to go to that page, get the source code from there, look for information or the next URL, and so on!\n",
    "\n",
    "Wellcome to the art of Web Scraping!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing HTLM code with RegEx\n",
    "\n",
    "Even if every page has a different code, there are patterns that stay the same. For every project we might want to search for similar information in very similar pages, therefore we can identify those code patterns that contain what we need and scrape it. There is no doubt that we need to learn now how to use RegEx or REGular EXpressions which is an obiquitous tool programming, independently of the language.\n",
    "\n",
    "Here we will implement this tool in Python to search my friends list for their facebook URL, because we want to first go to their profile, then to their info-page, and find the date of birth. So here we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This imports the python regex library, if you are already familiar with this concept you can move own and are ready to do your own scraping now, otherwise stick around and see how exactly we can get the info we want. First we need to make a couple of examples to explain how parsing works.\n",
    "\n",
    "Let's pretend that we want to find the important dates and people names in the following text:\n",
    "\n",
    "*DNA was first isolated by Friedrich Miescher in 1869. Its molecular structure was identified by James Watson and Francis Crick from Cold Spring Harbor Laboratory in 1953, whose model-building efforts were guided by X-ray diffraction data acquired by Raymond Gosling, who was a post-graduate student of Rosalind Franklin.*\n",
    "\n",
    "We want to isolate the numbers and names, here is how we could start doing that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1869', '1953']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'DNA was first isolated by Friedrich Miescher in 1869.   \\\n",
    "        Its molecular structure was identified by James Watson  \\\n",
    "        and Francis Crick from Cold Spring Harbor Laboratory in \\\n",
    "        1953, whose model-building efforts were guided by X-ray \\\n",
    "        diffraction data acquired by Raymond Gosling, who was a \\\n",
    "        post-graduate student of Rosalind Franklin.'\n",
    "dates = re.findall('\\d{4}', text)\n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used the function re.findall(*pattern*, *string*), where we looked for the pattern *\\d{4}* in the string contained in *text*. Here we used one of the regex **identifiers** (\\d) which identifies digits, followed by the **modifier** ({4}) which specifies how many digits in a row constitutes our pattern. And the result is what we wanted.\n",
    "\n",
    "If we now wanted to find the names of people we would do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Friedrich Miescher ',\n",
       " 'James Watson ',\n",
       " 'Francis Crick ',\n",
       " 'Cold Spring ',\n",
       " 'Harbor Laboratory ',\n",
       " 'Raymond Gosling,',\n",
       " 'Rosalind Franklin.']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = re.findall('[A-Z][a-z]*\\s[A-Z][a-z]*\\W', text)\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pattern here is more complicated, but can be broken down like this:  \n",
    "**[A-Z]**   &nbsp;&nbsp; any capital letter followed by  \n",
    "**[a-z]**   &nbsp;&nbsp;&nbsp; any lower case letter with the modifier  \n",
    "**\\***      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; as many repetitions (of lower case letters)  \n",
    "**\\s**      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; a space  \n",
    "**[A-Z][a-z]\\***  &nbsp;&nbsp;&nbsp;&nbsp; anther word that starts with a capital letter  \n",
    "**\\W**      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; followed by any non-character\n",
    "\n",
    "For a full tutorial on Regex see: https://www.youtube.com/watch?v=sZyAn2TW7GY\n",
    "\n",
    "Notice that this code successfully avoids counting the capitalized words at the beginning of each frase, but fails in recognizing that **Cold Spring Harbor Laboratory** is not a person name. There are other clever things that can be implemented to clean this list programmatically, but goes beyond the scope of this brief tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving the right URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To be continued!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
